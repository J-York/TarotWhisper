# 内置 LLM 配置（可选）
# 如果用户没有提供自己的 API 配置，将使用这些作为后备
# 在 Vercel 部署时，请在项目设置中添加这些环境变量

# 内置 LLM API 端点
FALLBACK_LLM_ENDPOINT=https://api.openai.com/v1/chat/completions

# 内置 LLM API 密钥
FALLBACK_LLM_KEY=your-api-key-here

# 内置 LLM 模型
FALLBACK_LLM_MODEL=gpt-4o-mini

# 是否启用内置配置（设置为 true 启用）
ENABLE_FALLBACK_LLM=false

# 速率限制配置（可选，防止滥用）
# 每个 IP 每小时最多请求次数
RATE_LIMIT_PER_HOUR=10
